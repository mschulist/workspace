---
title: 'Statistical Rethinking: Chapter 4'
author: "Mark Schulist"
date: "`r Sys.Date()`"
output: rmdformats::downcute
downcute_theme: "chaos"
---

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(rethinking)
library(tidyverse)
```

# Normal (Gaussian) Distributions

Things in the world tend to be normal, so we can use Gaussian Distributions to model the world. 

A distribution of sums tends to converge to a Gaussian distribution. 

```{r}
pos <- replicate(1000, sum(runif(16, -1, 1)))
dens(pos)
```


# Describing Models

Types of variables:

* Observable: data
* Unobservable (rates, averages): parameters

Variables are defined in terms of other variables or probability distributions. 

Combining variables and their probability distributions defines a *joint generative model* that can simulate hypothetical observations and analyze real observations. 

**Stochastic** = mapping a variable or parameter onto a distribution (with a `~` in the model definition)

# Gaussian Model of Height

Loading data. 
```{r}
data(Howell1)
d <- Howell1
```

Only keeping adults (using tidyverse since I'm gen z) and graphing density of the heights. 
```{r}
d2 <- d %>% filter(age > 18)
dens(d2$height)
```

Our model for the data (Normal can also be written as $N$): 

$$
\begin{align}
h_i &\sim \text{Normal}(\mu, \sigma)  &[\text{likelihood}] \\
\mu &\sim \text{Normal}(178, 20)  &[\mu \text{ prior}] \\
\sigma &\sim \text{Uniform}(0, 50)  &[\sigma \text{ prior}]
\end{align}
$$

Let's plot the priors for a reality check. 
```{r}
curve(dnorm(x, 178, 20), from = 100, to = 250, main = "mu prior distribution")
curve(dunif(x, 0, 50), from = -10, to = 60, main = "sigma prior distribution")
```

Let's do a *prior predictive simulation* to see the joint probs of the priors to see if it makes sensible choices about height. 
```{r}
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
```

It's OKAY that it is NOT a Gaussian distribution. "The distribution you see is not an empirical expectation, but rather the distribution of relative plausibilities of different heights, before seeing the data."

All that it shows us is that the priors make sense, which they do in this case. If we increased sigma to a larger value, we would end up with a significant part of the distribution in negative heights, which obviously does not make any sense. 

Using a grid approximation of the posterior. This is a dumb method since it is inefficient, but we'll get to the better methods later...
```{r}
mu.list <- seq(from = 150, to = 160, length.out = 500)
sigma.list <- seq(from = 7, to = 9, length.out = 500)
post <- expand.grid(mu = mu.list, sigma = sigma.list)
post$LL <- sapply(1:nrow(post), function(i) {
  sum(
    dnorm(d2$height, post$mu[i], post$sigma[i], log = TRUE)
  )
})
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) +
  dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))
```

We can sample from this posterior. 
```{r}
sample.rows <- sample(1:nrow(post),
  size = 1e4, replace = TRUE,
  prob = post$prob
)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
```

And then plot sampled posterior. 
```{r}
plot(sample.mu, sample.sigma, cex = .7, pch = 16, col = col.alpha(rangi2, 0.15))
```

The *marginal* posterior densities of $\mu$ and $\sigma$.
```{r}
dens(sample.mu)
dens(sample.sigma)
```

Let's quickly redo this model with less data to see how it affects the posterior. 
```{r}
d3 <- sample(d2$height, size = 20)
mu.list <- seq(from = 150, to = 170, length.out = 200)
sigma.list <- seq(from = 4, to = 20, length.out = 200)
post2 <- expand.grid(mu = mu.list, sigma = sigma.list)
post2$LL <- sapply(1:nrow(post2), function(i) {
  sum(dnorm(d3,
    mean = post2$mu[i], sd = post2$sigma[i],
    log = TRUE
  ))
})
post2$prod <- post2$LL + dnorm(post2$mu, 178, 20, TRUE) +
  dunif(post2$sigma, 0, 50, TRUE)
post2$prob <- exp(post2$prod - max(post2$prod))
sample2.rows <- sample(1:nrow(post2),
  size = 1e4, replace = TRUE,
  prob = post2$prob
)
sample2.mu <- post2$mu[sample2.rows]
sample2.sigma <- post2$sigma[sample2.rows]
plot(sample2.mu, sample2.sigma,
  cex = 0.5,
  col = col.alpha(rangi2, 0.1),
  xlab = "mu", ylab = "sigma", pch = 16
)

dens(sample2.sigma, norm.comp = TRUE)
```
Now, we can see that $\sigma$ is no longer Gaussian. Instead, it has a long tail to the right. 

# Using Quadratic Approximation

We input a model and it returns an approximation of the posterior. 

```{r}
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)
```

We can then fit this model with the data from `data(Howell1)` (heights/other stats of people)
```{r}
m4.1 <- quap(flist, d2)
precis(m4.1)
```

This provides us with the approximations for each parameter's marginal distribution. A distribution of distributions, lol. 

## Sampling from a `Quap`

We first examine the covariances among all pairs of parameters. This is known as the *Variance-covariance Matrix*. 
```{r}
vcov(m4.1)
```

We can decompose this into 1) a vector of the variances of the parameters and 2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in the others. 
```{r}
diag(vcov(m4.1))
cov2cor(vcov(m4.1))
```

The "1's" show that the values are correlated with themselves, which is what we want. 

The other values tell us that learning about mu doesn't tell us much about sigma or visa versa. 

To sample from a quap, we sample from each of the two parameters. There are two dimensions, so we sample for each dimension. 
```{r}
post <- extract.samples(m4.1, n = 1e4)
head(post)
```


We can plot these values, which should be *very* similar to the complex grid sampling model above. 
```{r}
dens(post$mu)
dens(post$sigma)
```

# Linear Prediction

We are normally interested in modeling how an outcome is related to some other variable, a *predictor value*. 

```{r}
plot(d2$height ~ d2$weight)
```

There's an obvious relationship. 

We want to make a *linear model* which says that there is a constant additive relationship to the mean and outcome. 

$$
\begin{align}
h_i &\sim \text{Normal}(\mu_1, \sigma) &[\text{likelihood}] \\
\mu_1 &= \alpha + \beta(x_i + \bar{x}) &[\text{linear model}] \\
\alpha &\sim \text{Normal}(178, 20) &[\alpha \text{ prior}] \\
\beta &\sim \text{Normal}(0, 10) &[\beta \text{ prior}] \\
\sigma &\sim \text{Uniform}(0, 50) &[\sigma \text{ prior}]
\end{align}
$$

The little i on mu indicates that there is a different mean for every data point. 

Mu is no longer estimated, it is determined by the linear model. 

Alpha is the intercept and Beta is the slope (rate of change in expectation). 

We will simulate the priors to see what they look like. 
```{r}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm(N, 178, 20)
b <- rnorm(N, 0, 10)

plot(NULL,
  xlim = range(d2$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2$weight)
for (i in 1:N) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2$weight), to = max(d2$weight), add = TRUE,
    col = col.alpha("black", 0.2)
  )
}
```

These priors don't really tell us anything. First, there are some negative values, and nobody has negative height. And there are some that are absurdly tall, so we can use a log(beta) to fix these problems. 

$$
\beta \sim \text{Log-Normal}(0, 1)
$$

`dlnorm` and `rlnorm` deal with log-normal distributions. 

```{r}
b <- rlnorm(1e4, 0, 1)
dens(b, xlim = c(0, 5), adj = 0.1)
```

Now using the prior predicitve simulation with a Log-Normal prior:
```{r}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm(N, 178, 20)
b <- rlnorm(N, 0, 1)
# plotting
plot(NULL,
  xlim = range(d2$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2$weight)
for (i in 1:N) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2$weight), to = max(d2$weight), add = TRUE,
    col = col.alpha("black", 0.2)
  )
}
```

We want to carefully pick our priors. They should be based on general facts, not the sample data. 

Let's remake the model, now with a log-normal distribution instead of a normal distribution. 

$$
\begin{align}
h_i &\sim \text{Normal}(\mu_1, \sigma) \\
\mu_1 &= \alpha + \beta(x_i + \bar{x}) \\
\alpha &\sim \text{Normal}(178, 20) \\
\beta &\sim \text{Log-Normal}(0, 1) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align}
$$


Getting the data ready for fitting to the new model.
```{r}
d2 <- Howell1 %>% filter(age >= 18)
```

Writing the parameters. 
```{r}
xbar <- mean(d2$weight)
# defining the model
m4.3 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - xbar),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)
```

We can look at tables, but often plotting the posterior is more useful than seeing the summary (marginal) stats. 
```{r}
precis(m4.3)
```

Beta is the slope, and because it is 0.9, it means, in context, that an increase of 1kg in weight corresponds with 0.9cm of height increase. (*a person 1 kg heavier is expected to be 0.90 cm taller*)

Variance-covariance Matrix for the data:
```{r}
vcov(m4.3) %>% round(3)
```

There is practically no relationship between the parameters.

Now we can plot the means for the parameters with the data. 
```{r}
plot(height ~ weight, data = d2, col = rangi2)
post <- extract.samples(m4.3)
a_map <- mean(post$a)
b_map <- mean(post$b)
curve(a_map + b_map * (x - xbar), add = TRUE)
```

Best fit lines are good, but they don't help us understand the uncertainty of that line. 

Let's extract the first 10 cases and then remodel and put the uncertainty in the graph. 

```{r}
N <- 10
dN <- d2 %>% slice_head(n = N)
mN <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - mean(weight)),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = dN
)
```

Let's extract 20 samples from this posterior.

```{r}
post <- extract.samples(mN, n = 20)
```

Now, let's plot the samples and the posterior. 
```{r}
# display raw data and sample size
plot(dN$weight, dN$height,
  xlim = range(d2$weight), ylim = range(d2$height),
  col = rangi2, xlab = "weight", ylab = "height"
)
mtext(concat("N = ", N))
# plot the lines, with transparency
for (i in 1:20) {
  curve(post$a[i] + post$b[i] * (x - mean(dN$weight)),
    col = col.alpha("black", 0.3), add = TRUE
  )
}
```

We can change `N` and the model will more certain about the mean (smaller variance) and the lines will be closer together. 

We can look more specifically at the value of mu. We can ask the following question: *What are the possible heights for a given weight?*

Let's try to answer that question for a weight of 50kg. 
```{r}
post <- extract.samples(m4.3)
mu_at_50 <- post$a + post$b * (50 - xbar)
dens(mu_at_50, col = rangi2, lwd = 2, xlab = "mu|weight=50")
```

Every parameter has a distribution, so here we are looking at the distribution of mu values for a given weight. 

We can find the 89% percentile interval for this above distribution. 
```{r}
PI(mu_at_50, prob = 0.89)
```

We can use the function `link` to take our quap approximation, sample from the posterior, and then compute mu for each case in the data and then sample from that posterior. It then returns a matrix where the rows are the samples and the columns are the different data points in the data. 
```{r}
mu <- link(m4.3)
```

We want to get a distribution of mu for each unique weight value on the horizontal axis. 
```{r}
weight.seq <- 25:70
mu <- link(m4.3, data = data.frame(weight = weight.seq))
```

Now, we have the corresponding distribution (density) of mu for weights in between 25 and 70. 
```{r}
plot(height ~ weight, d2, type = "n")
# looping over samples and plotting each mu value
for (i in 1:100) {
  points(weight.seq, mu[i, ], pch = 16, col = col.alpha(rangi2, 0.1))
}
```

Now, we just need to summarize these points and show how they relate to the data we used in the model. 
```{r}
# summarize the distribution of mu
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)

# plot raw data
# fading out points to make line and interval more visible
plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.5))
# plot the MAP line, aka the mean mu for each weight
lines(weight.seq, mu.mean)
# plot a shaded region for 89% PI
shade(mu.PI, weight.seq)
```

## Prediction Intervals

Let's say that we want to get the 89% prediction interval for actual heights, not just the average height (mu). That means we need to incorporate the sd (sigma) and its uncertainty as well. 

The entire reason that we use a Gaussian dist. is because it allows us to model the expected heights to be distributed around mu with some sd sigma, not just right on top of mu. In other words, mu does not have all of the information from the model. 

We can use the function `sim` to sample from the posterior for both parameters (mu and sigma). 
```{r}
sim.height <- sim(m4.3, data = list(weight = weight.seq))
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
```

Then we can plot the following:

1. Average line
2. Shaded region of 89% plausible mu
3. Boundaries of the simulated heights the model expects

```{r}
# plot raw data
plot(height ~ weight, d2, col = col.alpha(rangi2, 0.5))
# draw MAP line
lines(weight.seq, mu.mean)
# draw HPDI region for line
shade(mu.PI, weight.seq)
# draw PI region for simulated heights
shade(height.PI, weight.seq)
```

# Curves from Lines

We can use polynomial regression to model relationships between variables. 
```{r}
plot(height ~ weight, d)
```

It's obvious that there's a nonlinear relationship because now we are including kids in the data. 

*The most common polynomial regression is a parabolic model of the mean:*

$$
u_i = \alpha + \beta_1x_i + \beta_2x^2_i
$$

We first need to standardize the predictor variable to reduce the chance of numerical glitches as we are taking a number to a large power. 

```{r}
d <- d %>%
  mutate(
    weight_s = (weight - mean(weight)) / sd(weight),
    weight_s2 = weight_s^2
  )
```

Now we can define the new polynomial model. 

$$
\begin{align}
h_i &\sim \text{Normal}(\mu_1, \sigma) \\
\mu_i &= \alpha + \beta_1x_1 + \beta_2x^2_i \\
\alpha &\sim \text{Normal}(178, 20) \\
\beta_1 &\sim \text{Log-Normal}(0, 1) \\
\beta_2 &\sim \text{Normal}(0, 1) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align}
$$

We are not using a log-normal prior for $\beta_2$ because it can be negative, but that can be confusing so polynomial models are bad in general. 

Now we can write this model and put it through `quap`. 
```{r}
m4.5 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d
)
precis(m4.5)
```

Let's plot the quadratic model fits and see what they are saying. 
```{r}
# organizing
weight.seq <- seq( from=-2.2 , to=2 , length.out=30 )
pred_dat <- list( weight_s=weight.seq , weight_s2=weight.seq^2 )
mu <- link( m4.5 , data=pred_dat )
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )
sim.height <- sim( m4.5 , data=pred_dat )
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )
# plotting
plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) )
lines( weight.seq , mu.mean )
shade( mu.PI , weight.seq )
shade( height.PI , weight.seq )
```

Fitting a linear model (cheating) using ggplot2. 
```{r}
ggplot(d, aes(weight_s, height)) +
  geom_point() +
  geom_smooth(method = "lm")
```

It's obvious that the quadratic model fits the data better, but does that mean it is a better model. No, just no. 

## Splines

We'll look at B-splines, where b = basis = component. 

Let's load in a new data set for freshness. 
```{r}
data(cherry_blossoms)
d <- cherry_blossoms
```

Let's plot the temp against the year to see if there is a relationship. 
```{r}
plot(temp ~ year, d)
```

