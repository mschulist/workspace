---
title: 'Statistical Rethinking: Chapter 4'
author: "Mark Schulist"
date: "`r Sys.Date()`"
output: rmdformats::downcute
downcute_theme: "chaos"
---

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(rethinking)
library(tidyverse)
```

# Normal (Gaussian) Distributions

Things in the world tend to be normal, so we can use Gaussian Distributions to model the world. 

A distribution of sums tends to converge to a Gaussian distribution. 

```{r}
pos <- replicate(1000, sum(runif(16, -1, 1)))
dens(pos)
```


# Describing Models

Types of variables:

* Observable: data
* Unobservable (rates, averages): parameters

Variables are defined in terms of other variables or probability distributions. 

Combining variables and their probability distributions defines a *joint generative model* that can simulate hypothetical observations and analyze real observations. 

**Stochastic** = mapping a variable or parameter onto a distribution (with a `~` in the model definition)

# Gaussian Model of Height

Loading data. 
```{r}
data(Howell1)
d <- Howell1
```

Only keeping adults (using tidyverse since I'm gen z) and graphing density of the heights. 
```{r}
d2 <- d %>% filter(age > 18)
dens(d2$height)
```

Our model for the data (Normal can also be written as $N$): 

$$
\begin{align}
h_i &\sim \text{Normal}(\mu, \sigma)  &[\text{likelihood}] \\
\mu &\sim \text{Normal}(178, 20)  &[\mu \text{ prior}] \\
\sigma &\sim \text{Uniform}(0, 50)  &[\sigma \text{ prior}]
\end{align}
$$

Let's plot the priors for a reality check. 
```{r}
curve(dnorm(x, 178, 20), from = 100, to = 250, main = "mu prior distribution")
curve(dunif(x, 0, 50), from = -10, to = 60, main = "sigma prior distribution")
```

Let's do a *prior predictive simulation* to see the joint probs of the priors to see if it makes sensible choices about height. 
```{r}
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
```

It's OKAY that it is NOT a Gaussian distribution. "The distribution you see is not an empirical expectation, but rather the distribution of relative plausibilities of different heights, before seeing the data."

All that it shows us is that the priors make sense, which they do in this case. If we increased sigma to a larger value, we would end up with a significant part of the distribution in negative heights, which obviously does not make any sense. 

Using a grid approximation of the posterior. This is a dumb method since it is inefficient, but we'll get to the better methods later...
```{r}
mu.list <- seq(from = 150, to = 160, length.out = 500)
sigma.list <- seq(from = 7, to = 9, length.out = 500)
post <- expand.grid(mu = mu.list, sigma = sigma.list)
post$LL <- sapply(1:nrow(post), function(i) {
  sum(
    dnorm(d2$height, post$mu[i], post$sigma[i], log = TRUE)
  )
})
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) +
  dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))
```

We can sample from this posterior. 
```{r}
sample.rows <- sample(1:nrow(post),
  size = 1e4, replace = TRUE,
  prob = post$prob
)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
```

And then plot sampled posterior. 
```{r}
plot(sample.mu, sample.sigma, cex = .7, pch = 16, col = col.alpha(rangi2, 0.15))
```

The *marginal* posterior densities of $\mu$ and $\sigma$.
```{r}
dens(sample.mu)
dens(sample.sigma)
```

Let's quickly redo this model with less data to see how it affects the posterior. 
```{r}
d3 <- sample(d2$height, size = 20)
mu.list <- seq(from = 150, to = 170, length.out = 200)
sigma.list <- seq(from = 4, to = 20, length.out = 200)
post2 <- expand.grid(mu = mu.list, sigma = sigma.list)
post2$LL <- sapply(1:nrow(post2), function(i) {
  sum(dnorm(d3,
    mean = post2$mu[i], sd = post2$sigma[i],
    log = TRUE
  ))
})
post2$prod <- post2$LL + dnorm(post2$mu, 178, 20, TRUE) +
  dunif(post2$sigma, 0, 50, TRUE)
post2$prob <- exp(post2$prod - max(post2$prod))
sample2.rows <- sample(1:nrow(post2),
  size = 1e4, replace = TRUE,
  prob = post2$prob
)
sample2.mu <- post2$mu[sample2.rows]
sample2.sigma <- post2$sigma[sample2.rows]
plot(sample2.mu, sample2.sigma,
  cex = 0.5,
  col = col.alpha(rangi2, 0.1),
  xlab = "mu", ylab = "sigma", pch = 16
)

dens(sample2.sigma, norm.comp = TRUE)
```
Now, we can see that $\sigma$ is no longer Gaussian. Instead, it has a long tail to the right. 

# Using Quadratic Approximation

We input a model and it returns an approximation of the posterior. 

```{r}
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)
```

We can then fit this model with the data from `data(Howell1)` (heights/other stats of people)
```{r}
m4.1 <- quap(flist, d2)
precis(m4.1)
```

This provides us with the approximations for each parameter's marginal distribution. A distribution of distributions, lol. 

## Sampling from a `Quap`

We first examine the covariances among all pairs of parameters. This is known as the *Variance-covariance Matrix*. 
```{r}
vcov(m4.1)
```

We can decompose this into 1) a vector of the variances of the parameters and 2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in the others. 
```{r}
diag(vcov(m4.1))
cov2cor(vcov(m4.1))
```

The "1's" show that the values are correlated with themselves, which is what we want. 

The other values tell us that learning about mu doesn't tell us much about sigma or visa versa. 

To sample from a quap, we sample from each of the two parameters. There are two dimensions, so we sample for each dimension. 
```{r}
post <- extract.samples(m4.1, n = 1e4)
head(post)
```


We can plot these values, which should be *very* similar to the complex grid sampling model above. 
```{r}
dens(post$mu)
dens(post$sigma)
```

# Linear Prediction

We are normally interested in modeling how an outcome is related to some other variable, a *predictor value*. 

```{r}
plot(d2$height ~ d2$weight)
```

There's an obvious relationship. 

We want to make a *linear model* which says that there is a constant additive relationship to the mean and outcome. 

$$
\begin{align}
h_i &\sim \text{Normal}(\mu_1, \sigma) &[\text{likelihood}] \\
\mu_1 &= \alpha + \beta(x_i + \bar{x}) &[\text{linear model}] \\
\alpha &\sim \text{Normal}(178, 20) &[\alpha \text{ prior}] \\
\beta &\sim \text{Normal}(0, 10) &[\beta \text{ prior}] \\
\sigma &\sim \text{Uniform}(0, 50) &[\sigma \text{ prior}]
\end{align}
$$

The little i on mu indicates that there is a different mean for every data point. 

Mu is no longer estimated, it is determined by the linear model. 

Alpha is the intercept and Beta is the slope (rate of change in expectation). 

We will simulate the priors to see what they look like. 
```{r}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm(N, 178, 20)
b <- rnorm(N, 0, 10)

plot(NULL,
  xlim = range(d2$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2$weight)
for (i in 1:N) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2$weight), to = max(d2$weight), add = TRUE,
    col = col.alpha("black", 0.2)
  )
}
```

These priors don't really tell us anything. First, there are some negative values, and nobody has negative height. And there are some that are absurdly tall, so we can use a log(beta) to fix these problems. 

$$
\beta \sim \text{Log-Normal}(0, 1)
$$

`dlnorm` and `rlnorm` deal with log-normal distributions. 

```{r}
b <- rlnorm(1e4, 0, 1)
dens(b, xlim = c(0, 5), adj = 0.1)
```

Now using the prior predicitve simulation with a Log-Normal prior:
```{r}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm(N, 178, 20)
b <- rlnorm(N, 0, 1)
# plotting
plot(NULL,
  xlim = range(d2$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2$weight)
for (i in 1:N) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2$weight), to = max(d2$weight), add = TRUE,
    col = col.alpha("black", 0.2)
  )
}
```

We want to carefully pick our priors. They should be based on general facts, not the sample data. 

Let's remake the model, now with a log-normal distribution instead of a normal distribution. 

$$
\begin{align}
h_i &\sim \text{Normal}(\mu_1, \sigma) \\
\mu_1 &= \alpha + \beta(x_i + \bar{x}) \\
\alpha &\sim \text{Normal}(178, 20) \\
\beta &\sim \text{Log-Normal}(0, 1) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align}
$$


Getting the data ready for fitting to the new model.
```{r}
d2 <- Howell1 %>% filter(age >= 18)
```

Writing the parameters. 
```{r}
xbar <- mean(d2$weight)
# defining the model
m4.3 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - xbar),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)
```

We can look at tables, but often plotting the posterior is more useful than seeing the summary (marginal) stats. 
```{r}
precis(m4.3)
```

Beta is the slope, and because it is 0.9, it means, in context, that an increase of 1kg in weight corresponds with 0.9cm of height increase. (*a person 1 kg heavier is expected to be 0.90 cm taller*)

Variance-covariance Matrix for the data:
```{r}
vcov(m4.3) %>% round(3)
```

There is practically no relationship between the parameters.

Now we can plot the means for the parameters with the data. 
```{r}
plot(height ~ weight, data = d2, col = rangi2)
post <- extract.samples(m4.3)
a_map <- mean(post$a)
b_map <- mean(post$b)
curve(a_map + b_map * (x - xbar), add = TRUE)
```

Best fit lines are good, but they don't help us understand the uncertainty of that line. 

Let's extract the first 10 cases and then remodel and put the uncertainty in the graph. 

```{r}
N <- 10
dN <- d2 %>% slice_head(n = N)
mN <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - mean(weight)),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = dN
)
```

Let's extract 20 samples from this posterior.

```{r}
post <- extract.samples(mN, n = 20)
```

Now, let's plot the samples and the posterior. 
```{r}
# display raw data and sample size
plot(dN$weight, dN$height,
  xlim = range(d2$weight), ylim = range(d2$height),
  col = rangi2, xlab = "weight", ylab = "height"
)
mtext(concat("N = ", N))
# plot the lines, with transparency
for (i in 1:20) {
  curve(post$a[i] + post$b[i] * (x - mean(dN$weight)),
    col = col.alpha("black", 0.3), add = TRUE
  )
}
```

We can change `N` and the model will more certain about the mean (smaller variance) and the lines will be closer together. 

We can look more specifically at the value of mu. We can ask the following question: *What are the possible heights for a given weight?*

Let's try to answer that question for a weight of 50kg. 
```{r}
post <- extract.samples(m4.3)
mu_at_50 <- post$a + post$b * (50 - xbar)
dens(mu_at_50, col = rangi2, lwd = 2, xlab = "mu|weight=50")
```

Every parameter has a distribution, so here we are looking at the distribution of mu values for a given weight. 

We can find the 89% percentile interval for this above distribution. 
```{r}
PI(mu_at_50, prob = 0.89)
```

