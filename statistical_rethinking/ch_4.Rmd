---
title: 'Statistical Rethinking: Chapter 4'
author: "Mark Schulist"
date: "`r Sys.Date()`"
output: rmdformats::downcute
downcute_theme: "chaos"
---

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(rethinking)
library(tidyverse)
```

# Normal (Gaussian) Distributions

Things in the world tend to be normal, so we can use Gaussian Distributions to model the world. 

A distribution of sums tends to converge to a Gaussian distribution. 

```{r}
pos <- replicate(1000, sum(runif(16, -1, 1)))
dens(pos)
```


# Describing Models

Types of variables:

* Observable: data
* Unobservable (rates, averages): parameters

Variables are defined in terms of other variables or probability distributions. 

Combining variables and their probability distributions defines a *joint generative model* that can simulate hypothetical observations and analyze real observations. 

**Stochastic** = mapping a variable or parameter onto a distribution (with a `~` in the model definition)

# Gaussian Model of Height

Loading data. 
```{r}
data(Howell1)
d <- Howell1
```

Only keeping adults (using tidyverse since I'm gen z) and graphing density of the heights. 
```{r}
d2 <- d %>% filter(age > 18)
dens(d2$height)
```

Our model for the data (Normal can also be written as $N$): 

$$
\begin{align}
h_i &\sim \text{Normal}(\mu, \sigma)  &[\text{likelihood}] \\
\mu &\sim \text{Normal}(178, 20)  &[\mu \text{ prior}] \\
\sigma &\sim \text{Uniform}(0, 50)  &[\sigma \text{ prior}]
\end{align}
$$

Let's plot the priors for a reality check. 
```{r}
curve(dnorm(x, 178, 20), from = 100, to = 250, main = "mu prior distribution")
curve(dunif(x, 0, 50), from = -10, to = 60, main = "sigma prior distribution")
```

Let's do a *prior predictive simulation* to see the joint probs of the priors to see if it makes sensible choices about height. 
```{r}
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
```

It's OKAY that it is NOT a Gaussian distribution. "The distribution you see is not an empirical expectation, but rather the distribution of relative plausibilities of different heights, before seeing the data."

All that it shows us is that the priors make sense, which they do in this case. If we increased sigma to a larger value, we would end up with a significant part of the distribution in negative heights, which obviously does not make any sense. 

Using a grid approximation of the posterior. This is a dumb method since it is inefficient, but we'll get to the better methods later...
```{r}
mu.list <- seq(from = 150, to = 160, length.out = 500)
sigma.list <- seq(from = 7, to = 9, length.out = 500)
post <- expand.grid(mu = mu.list, sigma = sigma.list)
post$LL <- sapply(1:nrow(post), function(i) {
  sum(
    dnorm(d2$height, post$mu[i], post$sigma[i], log = TRUE)
  )
})
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) +
  dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))
```

We can sample from this posterior. 
```{r}
sample.rows <- sample(1:nrow(post),
  size = 1e4, replace = TRUE,
  prob = post$prob
)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
```

And then plot sampled posterior. 
```{r}
plot(sample.mu, sample.sigma, cex = .7, pch = 16, col = col.alpha(rangi2, 0.15))
```

The *marginal* posterior densities of $\mu$ and $\sigma$.
```{r}
dens(sample.mu)
dens(sample.sigma)
```

Let's quickly redo this model with less data to see how it affects the posterior. 
```{r}
d3 <- sample(d2$height, size = 20)
mu.list <- seq(from = 150, to = 170, length.out = 200)
sigma.list <- seq(from = 4, to = 20, length.out = 200)
post2 <- expand.grid(mu = mu.list, sigma = sigma.list)
post2$LL <- sapply(1:nrow(post2), function(i) {
  sum(dnorm(d3,
    mean = post2$mu[i], sd = post2$sigma[i],
    log = TRUE
  ))
})
post2$prod <- post2$LL + dnorm(post2$mu, 178, 20, TRUE) +
  dunif(post2$sigma, 0, 50, TRUE)
post2$prob <- exp(post2$prod - max(post2$prod))
sample2.rows <- sample(1:nrow(post2),
  size = 1e4, replace = TRUE,
  prob = post2$prob
)
sample2.mu <- post2$mu[sample2.rows]
sample2.sigma <- post2$sigma[sample2.rows]
plot(sample2.mu, sample2.sigma,
  cex = 0.5,
  col = col.alpha(rangi2, 0.1),
  xlab = "mu", ylab = "sigma", pch = 16
)

dens(sample2.sigma, norm.comp = TRUE)
```
Now, we can see that $\sigma$ is no longer Gaussian. Instead, it has a long tail to the right. 

# Using Quadratic Approximation

We input a model and it returns an approximation of the posterior. 

```{r}
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)
```

We can then fit this model with the data from `data(Howell1)` (heights/other stats of people)
```{r}
m4.1 <- quap(flist, d2)
precis(m4.1)
```

This provides us with the approximations for each parameter's marginal distribution. A distribution of distributions, lol. 

## Sampling from a `Quap`

We first examine the covariances among all pairs of parameters. This is known as the *Variance-covariance Matrix*. 
```{r}
vcov(m4.1)
```

We can decompose this into 1) a vector of the variances of the parameters and 2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in the others. 
```{r}
diag(vcov(m4.1))
cov2cor(vcov(m4.1))
```

The "1's" show that the values are correlated with themselves, which is what we want. 

The other values tell us that learning about mu doesn't tell us much about sigma or visa versa. 

To sample from a quap, we sample from each of the two parameters. There are two dimensions, so we sample for each dimension. 
```{r}
post <- extract.samples(m4.1, n = 1e4)
head(post)
```


We can plot these values, which should be *very* similar to the complex grid sampling model above. 
```{r}
dens(post$mu)
dens(post$sigma)
```

# Linear Prediction

We are normally interested in modeling how an outcome is related to some other variable, a *predictor value*. 

```{r}
plot(d2$height ~ d2$weight)
```

There's an obvious relationship. 

We want to make a *linear model* which says that there is a constant additive relationship to the mean and outcome. 

$$
\begin{align}
h_i &\sim \text{Normal}(\mu_1, \sigma) &[\text{likelihood}] \\
\mu_1 &= \alpha + \beta(x_i + \bar{x}) &[\text{linear model}] \\
\alpha &\sim \text{Normal}(178, 20) &[\alpha \text{ prior}] \\
\beta &\sim \text{Normal}(0, 10) &[\beta \text{ prior}] \\
\sigma &\sim \text{Uniform}(0, 50) &[\sigma \text{ prior}]
\end{align}
$$

The little i on mu indicates that there is a different mean for every data point. 

Mu is no longer estimated, it is determined by the linear model. 

Alpha is the intercept and Beta is the slope (rate of change in expectation). 

We will simulate the priors to see what they look like. 
```{r}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm(N, 178, 20)
b <- rnorm(N, 0, 10)

plot(NULL,
  xlim = range(d2$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2$weight)
for (i in 1:N) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2$weight), to = max(d2$weight), add = TRUE,
    col = col.alpha("black", 0.2)
  )
}
```

These priors don't really tell us anything. First, there are some negative values, and nobody has negative height. And there are some that are absurdly tall, so we can use a log(beta) to fix these problems. 

$$
\beta \sim \text{Log-Normal}(0, 1)
$$

`dlnorm` and `rlnorm` deal with log-normal distributions. 

```{r}
b <- rlnorm(1e4, 0, 1)
dens(b, xlim = c(0, 5), adj = 0.1)
```

Now using the prior predicitve simulation with a Log-Normal prior:
```{r}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm(N, 178, 20)
b <- rlnorm(N, 0, 1)
# plotting
plot(NULL,
  xlim = range(d2$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2$weight)
for (i in 1:N) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2$weight), to = max(d2$weight), add = TRUE,
    col = col.alpha("black", 0.2)
  )
}
```

We want to carefully pick our priors. They should be based on general facts, not the sample data. 

Let's remake the model, now with a log-normal distribution instead of a normal distribution. 

$$
\begin{align}
h_i &\sim \text{Normal}(\mu_1, \sigma) \\
\mu_1 &= \alpha + \beta(x_i + \bar{x}) \\
\alpha &\sim \text{Normal}(178, 20) \\
\beta &\sim \text{Log-Normal}(0, 1) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align}
$$


Getting the data ready for fitting to the new model.
```{r}
d2 <- Howell1 %>% filter(age >= 18)
```

Writing the parameters. 
```{r}
xbar <- mean(d2$weight)
# defining the model
m4.3 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - xbar),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)
```

We can look at tables, but often plotting the posterior is more useful than seeing the summary (marginal) stats. 
```{r}
precis(m4.3)
```

Beta is the slope, and because it is 0.9, it means, in context, that an increase of 1kg in weight corresponds with 0.9cm of height increase. (*a person 1 kg heavier is expected to be 0.90 cm taller*)

Variance-covariance Matrix for the data:
```{r}
vcov(m4.3) %>% round(3)
```

There is practically no relationship between the parameters.

Now we can plot the means for the parameters with the data. 
```{r}
plot(height ~ weight, data = d2, col = rangi2)
post <- extract.samples(m4.3)
a_map <- mean(post$a)
b_map <- mean(post$b)
curve(a_map + b_map * (x - xbar), add = TRUE)
```

Best fit lines are good, but they don't help us understand the uncertainty of that line. 

Let's extract the first 10 cases and then remodel and put the uncertainty in the graph. 

```{r}
N <- 10
dN <- d2 %>% slice_head(n = N)
mN <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - mean(weight)),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = dN
)
```

Let's extract 20 samples from this posterior.

```{r}
post <- extract.samples(mN, n = 20)
```

Now, let's plot the samples and the posterior. 
```{r}
# display raw data and sample size
plot(dN$weight, dN$height,
  xlim = range(d2$weight), ylim = range(d2$height),
  col = rangi2, xlab = "weight", ylab = "height"
)
mtext(concat("N = ", N))
# plot the lines, with transparency
for (i in 1:20) {
  curve(post$a[i] + post$b[i] * (x - mean(dN$weight)),
    col = col.alpha("black", 0.3), add = TRUE
  )
}
```

We can change `N` and the model will more certain about the mean (smaller variance) and the lines will be closer together. 

We can look more specifically at the value of mu. We can ask the following question: *What are the possible heights for a given weight?*

Let's try to answer that question for a weight of 50kg. 
```{r}
post <- extract.samples(m4.3)
mu_at_50 <- post$a + post$b * (50 - xbar)
dens(mu_at_50, col = rangi2, lwd = 2, xlab = "mu|weight=50")
```

Every parameter has a distribution, so here we are looking at the distribution of mu values for a given weight. 

We can find the 89% percentile interval for this above distribution. 
```{r}
PI(mu_at_50, prob = 0.89)
```

We can use the function `link` to take our quap approximation, sample from the posterior, and then compute mu for each case in the data and then sample from that posterior. It then returns a matrix where the rows are the samples and the columns are the different data points in the data. 
```{r}
mu <- link(m4.3)
```

We want to get a distribution of mu for each unique weight value on the horizontal axis. 
```{r}
weight.seq <- 25:70
mu <- link(m4.3, data = data.frame(weight = weight.seq))
```

Now, we have the corresponding distribution (density) of mu for weights in between 25 and 70. 
```{r}
plot(height ~ weight, d2, type = "n")
# looping over samples and plotting each mu value
for (i in 1:100) {
  points(weight.seq, mu[i, ], pch = 16, col = col.alpha(rangi2, 0.1))
}
```

Now, we just need to summarize these points and show how they relate to the data we used in the model. 
```{r}
# summarize the distribution of mu
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)

# plot raw data
# fading out points to make line and interval more visible
plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.5))
# plot the MAP line, aka the mean mu for each weight
lines(weight.seq, mu.mean)
# plot a shaded region for 89% PI
shade(mu.PI, weight.seq)
```

## Prediction Intervals

Let's say that we want to get the 89% prediction interval for actual heights, not just the average height (mu). That means we need to incorporate the sd (sigma) and its uncertainty as well. 

The entire reason that we use a Gaussian dist. is because it allows us to model the expected heights to be distributed around mu with some sd sigma, not just right on top of mu. In other words, mu does not have all of the information from the model. 

We can use the function `sim` to sample from the posterior for both parameters (mu and sigma). 
```{r}
sim.height <- sim(m4.3, data = list(weight = weight.seq))
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
```

Then we can plot the following:

1. Average line
2. Shaded region of 89% plausible mu
3. Boundaries of the simulated heights the model expects

```{r}
# plot raw data
plot(height ~ weight, d2, col = col.alpha(rangi2, 0.5))
# draw MAP line
lines(weight.seq, mu.mean)
# draw HPDI region for line
shade(mu.PI, weight.seq)
# draw PI region for simulated heights
shade(height.PI, weight.seq)
```

# Curves from Lines

We can use polynomial regression to model relationships between variables. 
```{r}
plot(height ~ weight, d)
```

It's obvious that there's a nonlinear relationship because now we are including kids in the data. 

*The most common polynomial regression is a parabolic model of the mean:*

$$
u_i = \alpha + \beta_1x_i + \beta_2x^2_i
$$

We first need to standardize the predictor variable to reduce the chance of numerical glitches as we are taking a number to a large power. 

```{r}
d <- d %>%
  mutate(
    weight_s = (weight - mean(weight)) / sd(weight),
    weight_s2 = weight_s^2
  )
```

Now we can define the new polynomial model. 

$$
\begin{align}
h_i &\sim \text{Normal}(\mu_1, \sigma) \\
\mu_i &= \alpha + \beta_1x_1 + \beta_2x^2_i \\
\alpha &\sim \text{Normal}(178, 20) \\
\beta_1 &\sim \text{Log-Normal}(0, 1) \\
\beta_2 &\sim \text{Normal}(0, 1) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align}
$$

We are not using a log-normal prior for $\beta_2$ because it can be negative, but that can be confusing so polynomial models are bad in general. 

Now we can write this model and put it through `quap`. 
```{r}
m4.5 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d
)
precis(m4.5)
```

Let's plot the quadratic model fits and see what they are saying. 
```{r}
# organizing
weight.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat <- list(weight_s = weight.seq, weight_s2 = weight.seq^2)
mu <- link(m4.5, data = pred_dat)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
sim.height <- sim(m4.5, data = pred_dat)
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
# plotting
plot(height ~ weight_s, d, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
```

Fitting a linear model (cheating) using ggplot2. 
```{r}
ggplot(d, aes(weight_s, height)) +
  geom_point() +
  geom_smooth(method = "lm")
```

It's obvious that the quadratic model fits the data better, but does that mean it is a better model. No, just no. 

## Splines

We'll look at B-splines, where b = basis = component. 

Let's load in a new data set for freshness. 
```{r}
data(cherry_blossoms)
d <- cherry_blossoms
```

Let's plot the temp against the year to see if there is a relationship. 
```{r}
plot(temp ~ year, d)
```

Yeah, there's no way a parabolic curve is going to fit this data!

We are going to "split up" these variables and then fit a parameter to each of the corresponding parts. 

This is what the linear model ends up looking like. 

$$
\mu_i = \alpha + w_1B_{i, 1} + w_2B_{i, 2} + w_3B_{i, 3} + \dots
$$

$B_{i, n}$ is the n-th basis function's value on row i, and $w$ are the corresponding weights for each. 

We can add more knots (knot = each starting point/division in the model) to make the spline more flexible to the data. 

Let's make a simple plot to show this model. 
```{r}
d2 <- d %>% filter(!is.na(temp))
num_knots <- 15
knot_list <- quantile(d2$year, probs = seq(0, 1, length.out = num_knots))
```

Now, we can make the basis functions. We will have 15 funs of degree 3 (so they will cubic)
```{r}
library(splines)
B <- bs(
  d2$year,
  knots = knot_list[-c(1, num_knots)],
  degree = 3, intercept = T
)
```

Now we plot them. 
```{r}
plot(NULL, xlim = range(d2$year), ylim = 0:1, xlab = "year", ylab = "basis value")
for (i in 1:ncol(B)) lines(d2$year, B[, i])
```

These are the basis funs. They don't have any weights, but that's what we'll do next. We first need to define the model we will use to get the weights. 

$$
\begin{align}
T_i &\sim \text{Normal}(\mu_1, \sigma) \\
\mu_1 &= \alpha + \sum_{k=1}^K w_k B_{k, i} \\
\alpha &\sim \text{Normal}(6, 10) \\
w_j &\sim \text{Normal}(0, 1) \\
\sigma &\sim \text{Exponential}(1)
\end{align}
$$

Defining the model to be estimated in quap. (`%*%` multiplies two matrices)
```{r}
m4.7 <- quap(
  alist(
    T ~ dnorm(mu, sigma),
    mu <- a + B %*% w,
    a ~ dnorm(6, 10),
    w ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = list(T = d2$temp, B = B),
  start = list(w = rep(0, ncol(B)))
)
```

We can view all of the parameters that it estimated. 
```{r}
precis(m4.7, depth = 2)
```

Let's plot the posterior (samples from it).
```{r}
post <- extract.samples(m4.7)
w <- apply(post$w, 2, mean)
plot(NULL,
  xlim = range(d2$year), ylim = c(-2, 2),
  xlab = "year", ylab = "basis * weight"
)
for (i in 1:ncol(B)) lines(d2$year, w[i] * B[, i])
```

Then we can add them together and find the 97% posterior interval. 
```{r}
mu <- link(m4.7)
mu_PI <- apply(mu, 2, PI, 0.97)
plot(d2$year, d2$temp, col = col.alpha(rangi2, 0.3), pch = 16)
shade(mu_PI, d2$year, col = col.alpha("black", 0.5))
```

The more knots we add, the more parameters it has to estimate and the closer our model fits the data, but of course that doesn't mean the more parameters == better model. 

# Practice

## Easy

$$
\begin{align}
y_i &\sim \text{Normal}(\mu, \sigma) \\
\mu &\sim \text{Normal}(0, 10) \\
\sigma &\sim \text{Exponential}(1)
\end{align}
$$

**4E1. In the model definition, which line is the likelihood?**

\[y_i \sim \text{Normal}(\mu, \sigma)\]

**4E2. In the model definition just above, how many parameters are in the posterior distribution?**

There are two parameters, $\mu$ and $\sigma$.

**4E3. Using the model definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.**

$$
\text{Pr}(\mu, \sigma | y) = \frac{\text{Pr}(y|\mu, \sigma)\text{Pr}(\mu)\text{Pr}(\sigma)} {\int \int \text{Pr}(y|\mu, \sigma)\text{Pr}(\mu)\text{Pr}(\sigma)d\mu d\sigma}
$$

Now using the distributions defined in the model. 

$$
\text{Pr}(\mu, \sigma | y) = \frac{\text{Normal}(\mu, \sigma) \cdot \text{Normal}(0, 10) \cdot \text{Exponential}(1)} {\int \int \text{Normal}(\mu, \sigma) \cdot \text{Normal}(0, 10) \cdot \text{Exponential}(1)d\mu d\sigma}
$$


**4E4. In the model definition below, which line is the linear model?**

$$
\begin{align}
y_i &\sim \text{Normal}(\mu, \sigma) \\
\mu_i &= \alpha + \beta x_i \\
\alpha &\sim \text{Normal}(0, 10) \\
\beta &\sim \text{Normal}(0, 1) \\
\sigma &\sim \text{Exponential}(2)
\end{align}
$$

The linear model is $\mu_i = \alpha + \beta x_i$ as that is the part that has the slope and intercept. 

**4E5. In the model definition just above, how many parameters are in the posterior distribution?**

There are 3, since mu is not a parameter as it is determined explicitly by a and b. 

## Medium

**4M1. For the model definition below, simulate observed y values from the prior (not the posterior).**

$$
\begin{align}
y_i &\sim \text{Normal}(\mu, \sigma) \\
\mu &\sim \text{Normal}(0, 10) \\
\sigma &\sim \text{Exponential}(1)
\end{align}
$$

```{r}
mu.sim <- rnorm(10000, 0, 10)
sigma.sim <- rexp(10000)
h.sim <- rnorm(10000, mu.sim, sigma.sim)
dens(h.sim)
```

**4M2. Translate the model just above into a quap formula.**

```{r}
m2 <- alist(
  y ~ dnorm(mu, sigma),
  mu ~ dnorm(0, 10),
  sigma ~ dexp(1)
)
```

**4M3. Translate the quap model formula below into a mathematical model definition.**

```
flist <- alist(
  y ~ dnorm(mu, sigma),
  mu <- a + b * x,
  a ~ dnorm(0, 10),
  b ~ dunif(0, 1),
  sigma ~ dexp(1)
)
```

$$
\begin{align}
y_i &\sim \text{Normal}(\mu, \sigma) \\
\mu &= \alpha + \beta x_i \\
\alpha &\sim \text{Normal}(0, 10) \\
\beta &\sim \text{Uniform}(0, 1) \\
\sigma &\sim \text{Exponential}(1)
\end{align}
$$

**4M4. A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.**

$$
\begin{align}
h_i &\sim \text{Normal}(\mu, \sigma) \\
\mu &= \alpha + \beta y_i \\
\alpha &\sim \text{Normal}(0, 50) \\
\beta &\sim \text{Uniform}(0, 10) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align}
$$

$\alpha$ prior means that the possible values for the intercept are normally distributed with a mean of 0 and a sd of 50. So it's basically uninformative but it will do if we have enough data. 

$\beta$ prior means that the slope ($\dfrac{\text{height}}{\text{year}}$) is positive and is somewhere between 0 and 10 with no preference for any value in this uniform distribution. 

$\sigma$ prior means that the sd for the distribution of heights is somewhere between 0 and 50, but we could probably improve this prior and other priors if we had information on the units used for height (cm, ...). 

**4M5. Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?**

We can try to change the $\alpha$ prior to include more information about this fact. Alpha is the intercept for the linear model, so we know that it must be positive. We can use this to set the mean higher, let's say 100?

**4M6. Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?**

We can change $\sigma$ to include this information. It will become $\sigma \sim \text{Uniform}(0, 64)$. 

## Hard

**4H1. The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals for each of these individuals. That is, fill in the table below, using model-based predictions.**

| Individual | Weight | expected height | 89% interval |
| ---------- |------- | --------------- | ------------ |
| 1          | 46.95  |                 |              |
| 2          | 43.72  |                 |              |
| 3          | 64.78  |                 |              |
| 4          | 32.59  |                 |              |
| 5          | 54.63  |                 |              | 

Defining the model in R:
```{r}
# data
data(Howell1)
d <- Howell1 %>% filter(age >= 18)
# model
m <- quap(
  alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + b * weight, 
    a ~ dnorm(75, 75),
    b ~ dunif(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = d
)
```

Then we need to get the samples from the model.
```{r}
post <- extract.samples(m)
```

Now we can plug in the weights to get the interval for each corresponding weight. 
```{r}
for (weight in c(46.95, 43.72, 64.78, 32.59, 54.63)) {
  height <- rnorm(1e4, post$a + post$b * weight, post$sigma)
  HPDI(height) %>% c(weight, .) %>% print()
}
```

We are basically finding the height using the posterior samples of the model and inputting each of the data points to find the posterior of that data point and then finding the 89% HPDI for that posterior. 

**4H2. Select out all the rows in the Howell1 data with ages below 18 years of age. If you do it right, you should end up with a new data frame with 192 rows in it.**

**1. Fit a linear regression to these data, using quap. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?**
**2. Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Superimpose the MAP regression line and 89% interval for the mean. Also superimpose the 89% interval for predicted heights.**
**3. What aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You don’t have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model.**

```{r}
# selecting the rows
d <- Howell1 %>% filter(age < 18)
```

```{r}
# writing the model
m <- quap(
  alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + b * weight, 
    a ~ dnorm(75, 75),
    b ~ dunif(0, 25),
    sigma ~ dunif(0, 50)
  ),
  data = d
)
precis(m)
```

For every 10 units of weight increase I expect the height to increase by (a mean of) 27.2 units. 

```{r}
# getting the 89% posterior interval using link
weight.seq <- min(d$weight):max(d$weight)
mu <- link(m, data = data.frame(weight = weight.seq))
```

```{r}
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.89)
sim.height <- sim(m, data = list(weight = weight.seq))
height.HPDI <- apply(sim.height, 2, HPDI, prob = 0.89)
```

Now plotting from the samples. 
```{r}
plot(height ~ weight, d)
lines(weight.seq, mu.mean)
shade(mu.HPDI, weight.seq)
shade(height.HPDI, weight.seq)
```

It obvious that this is not a good fit. The data should not be modeled using a linear model, and should probably use a quadratic or (better yet) a spline. A quadratic would not be useful outside of the bounds of the data whereas a spline would be useful outside (if done correctly).

**4H3. Suppose a colleague of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the logarithm of body weight that scales with height!” Let’s take your colleague’s advice and see what happens.**

1. Model the relationship between height (cm) and the natural logarithm of weight (log-kg). Use the entire Howell1 data frame, all 544 rows, adults and non-adults. Fit this model, using quadratic approximation:

$$
\begin{align}
h_i &\sim \text{Normal}(\mu, \sigma) \\
\mu_i &= \alpha + \beta \log(w_i) \\
\alpha &\sim \text{Normal}(178, 20) \\
\beta &\sim \text{Log-Normal}(0, 1) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align}
$$
Writing in R:
```{r}
d <- Howell1
model <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * log(weight),
    a ~ dnorm(178, 20), 
    b ~ dnorm(0, 100), 
    sigma ~ dunif(0, 50)
  ),
  data = d
)
precis(model)
```

`a` is the intercept, so the height when someone doesn't weigh anything. 

`b` is the log-kg slope of height/weight. So for every increase of 1 log-kg of weight, you expect an increase of 46.83 cm of height. 

